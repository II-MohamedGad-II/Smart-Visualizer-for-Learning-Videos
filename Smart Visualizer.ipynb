{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Load DeepSeek-Coder-instruct model & tokenizer\n",
    "MODEL_NAME = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "notebook_login()\n",
    "\n",
    "tokenizer_ds = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model_ds = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True).cuda()\n",
    "\n",
    "def extract_audio_to_numpy(video_path):\n",
    "    out, _ = (\n",
    "        ffmpeg\n",
    "        .input(video_path)\n",
    "        .output('pipe:', format='f32le', acodec='pcm_f32le', ac=1, ar='16000')\n",
    "        .run(capture_stdout=True, capture_stderr=True)\n",
    "    )\n",
    "    audio = np.frombuffer(out, np.float32)\n",
    "    return audio\n",
    "\n",
    "def transcribe_video_in_memory(video_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    audio = extract_audio_to_numpy(video_path)\n",
    "    result = model.transcribe(audio, fp16=False)  # Set fp16=True if using GPU\n",
    "    return result\n",
    "\n",
    "def filter_segments(segments):\n",
    "    filtered = []\n",
    "    new = {}\n",
    "    for segment in segments:\n",
    "        new = {\n",
    "            'id': segment['id'],\n",
    "            'start': segment['start'],\n",
    "            'end': segment['end'],\n",
    "            'text': segment['text']\n",
    "        }\n",
    "        filtered.append(new)\n",
    "    return filtered\n",
    "\n",
    "def chooseMoments(segments, number_of_images):\n",
    "    # Build a single prompt string combining system + user instructions\n",
    "    system_instruction = (\n",
    "        \"You are an expert multimedia content creator helping convert educational \"\n",
    "        \"videos into engaging visual formats.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"You are given a list of video transcript segments, each with a start time, \"\n",
    "        f\"end time, and the spoken text. Your task is to analyze the content of each \"\n",
    "        f\"segment and select the best {number_of_images} moments where an image could \"\n",
    "        f\"be generated to visually represent the idea, scene, or concept being discussed.\\n\\n\"\n",
    "        \"Return a JSON array of exactly \"\n",
    "        f\"{number_of_images} objects with keys: id, text, start, end, image_suggestion. \"\n",
    "        f\"Avoid transitions—focus on visually rich segments.\\n\\nSegments:\\n\"\n",
    "        f\"{segments}\"\n",
    "    )\n",
    "\n",
    "    # Apply DeepSeek chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    inputs = tokenizer_ds.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model_ds.device)\n",
    "\n",
    "    output_ids = model_ds.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer_ds.eos_token_id,\n",
    "    )\n",
    "    output = tokenizer_ds.decode(output_ids[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_json_from_string(text, result):\n",
    "    match = re.search(r\"```json\\s*(\\[.*?\\])\\s*```\", text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "        try:\n",
    "            x = json.loads(json_str)\n",
    "            x.append({'text': result['text']})\n",
    "            return x\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Found JSON block but failed to parse it: {e}\")\n",
    "    else:\n",
    "        raise ValueError(\"No JSON array found in the input text.\")\n",
    "    \n",
    "    \n",
    "def overlay_image_on_video(video_path, img, start_time, end_time):\n",
    "    from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "    from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip\n",
    "    from moviepy.video.VideoClip import ImageClip\n",
    "    from PIL import Image\n",
    "    video = VideoFileClip(video_path)\n",
    "    duration = video.duration\n",
    "\n",
    "    if start_time >= end_time or end_time > duration:\n",
    "        raise ValueError(\"Invalid time range. Ensure 0 <= start < end <= video duration.\")\n",
    "\n",
    "    # Resize image to match video height\n",
    "    aspect_ratio = img.width / img.height\n",
    "    new_height = video.h\n",
    "    new_width = int(aspect_ratio * new_height)\n",
    "    resized_img = img.resize((new_width, new_height))\n",
    "    resized_img_path = \"resized_overlay_temp.png\"\n",
    "    resized_img.save(resized_img_path)\n",
    "\n",
    "    # Create image clip and configure timing and position\n",
    "    image = ImageClip(resized_img_path)\n",
    "    image.start = start_time\n",
    "    image.end = end_time\n",
    "    image.pos = lambda t: ((video.w - image.w) // 2, (video.h - image.h) // 2)\n",
    "\n",
    "    final_video = CompositeVideoClip([video, image])\n",
    "    return final_video\n",
    "\n",
    "\n",
    "def GenerateImagesFromPrompts(BestMoments, video_path, number_of_images=1):\n",
    "    from diffusers import DiffusionPipeline\n",
    "    from moviepy import VideoFileClip, CompositeVideoClip, ImageClip\n",
    "    from PIL import Image\n",
    "\n",
    "    # Load the base video once\n",
    "    video = VideoFileClip(video_path)\n",
    "\n",
    "    # Load the diffusion model\n",
    "    pipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "\n",
    "    # Start with base video\n",
    "    clips = [video]\n",
    "\n",
    "    # Loop through each valid moment\n",
    "    for moment in BestMoments:\n",
    "        if len(moment) < 4 or 'image_suggestion' not in moment:\n",
    "            continue\n",
    "\n",
    "        # Generate image\n",
    "        prompt = moment['image_suggestion']\n",
    "        img = pipe(prompt).images[0]  # PIL.Image\n",
    "\n",
    "        # Resize image to match video height\n",
    "        aspect_ratio = img.width / img.height\n",
    "        new_height = video.h\n",
    "        new_width = int(aspect_ratio * new_height)\n",
    "        resized_img = img.resize((new_width, new_height))\n",
    "        resized_img_path = \"resized_overlay_temp.png\"\n",
    "        resized_img.save(resized_img_path)\n",
    "\n",
    "        # Create image clip using your preferred method\n",
    "        image = ImageClip(resized_img_path)\n",
    "        image.start = moment['start']\n",
    "        image.end = moment['end']\n",
    "        image.pos = lambda t: ((video.w - image.w) // 2, (video.h - image.h) // 2)\n",
    "\n",
    "        # Add image overlay to clips\n",
    "        clips.append(image)\n",
    "        print(f\"✅ {len(clips)} image generated and overlayed on video for moment\")\n",
    "\n",
    "    # Combine all into one final video\n",
    "    final = CompositeVideoClip(clips)\n",
    "    return final, clips\n",
    "    \n",
    "def main(video_file, number_of_images=2):\n",
    "    result = transcribe_video_in_memory(video_file)\n",
    "    segments = filter_segments(result['segments'])\n",
    "    print(\"✅ Transcription and Filteration is Done.\\n      Now Choosing Best Moments for Image Generation...\")\n",
    "\n",
    "    x = chooseMoments(segments, number_of_images)\n",
    "    BestMoments = extract_json_from_string(x, result)\n",
    "\n",
    "    print(f\"✅ BestMoments Are {BestMoments}.\\n==========================\\n      Now Generating Images and Overlaying on Video...\")\n",
    "    final_video, clips = GenerateImagesFromPrompts(BestMoments, video_file, number_of_images)\n",
    "\n",
    "    print(\"✅ Images Generated and Video is ready.✅\")\n",
    "    return final_video, clips\n",
    "\n",
    "video_file = \"path/to/video.mp4\"\n",
    "final_clip = main(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cc9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
